<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Hugo 0.16-DEV" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">
  <link rel="stylesheet" href="../css/custom.css">
  <link rel="alternate" href="index.xml" type="application/rss+xml" title="NEXTLab">
  <link rel="shortcut icon" href="favicon.png" type="image/x-icon" />
  <title>Bridging Categorical and Dimensional Affect: The MVEmo Multi-Task Benchmark for Music-Related Emotion Recognition</title>
</head>

<body>

  <div class="container">

    <main role="main">
      <article itemscope itemtype="https://schema.org/BlogPosting">
        <h1 class="entry-title" itemprop="headline">Bridging Categorical and Dimensional Affect: The MVEmo Multi-Task Benchmark for Music-Related Emotion Recognition
        </h1>

        <section itemprop="entry-text">
          <br>
          <h2 id="authors">Authors</h2>
          <ul>
            <li>Jiaxing Yu (Zhejiang University) <a href="mailto:yujx@zju.edu.cn">yujx@zju.edu.cn</a></li>
            <li>Ziyi Huang (Zhejiang University) <a href="mailto:ziyihuang1016@zju.edu.cn">ziyihuang1016@zju.edu.cn</a></li>
            <li>Shuyu Li (Zhejiang University) <a href="mailto:lsyxary@zju.edu.cn">lsyxary@zju.edu.cn</a></li>
            <li>Songruoyao Wu (Zhejiang University) <a href="mailto:wsry@zju.edu.cn">wsry@zju.edu.cn</a></li>
            <li>Shulei Ji (Zhejiang University; Innovation Center of Yangtze River Delta, Zhejiang University) <a href="mailto:shuleiji@zju.edu.cn">shuleiji@zju.edu.cn</a></li>
            <li>Kejun Zhang* (Zhejiang University; Innovation Center of Yangtze River Delta, Zhejiang University), Member, IEEE <a href="mailto:zhangkejun@zju.edu.cn">zhangkejun@zju.edu.cn</a></li>
          </ul>
          <p><small>* Corresponding Author</small></p>
          <h2 id="abstract">Abstract</h2>
          <div style="text-align:left">
            <p>Music-related emotion recognition (MRER) aims to automatically predict emotional states based on different musical forms (e.g., lyrics, music, and music videos). Despite notable advancements in the field, MRER still faces the following challenges: 1) heterogeneity in emotion representations across datasets, including categorical and dimensional labels; 2) scarcity of comprehensive modalities and emotion annotations (e.g., static and dynamic) in existing music video emotion datasets; and 3) absence of a benchmark that contains various tasks and evaluation metrics for MRER. In this paper, we first propose a unified emotion representation consisting of emotion category and intensity, along with correlated conversion strategies to integrate disparate labels. Building upon the unified representation, we introduce an innovative emotion annotation framework MVAnno, which employs a hierarchical continual fine-tuning process on multiple modalities to obtain accurate emotion annotations. We also construct a large-scale music video emotion dataset MVEmo, which comprises 11K samples, with 11K static emotion labels and 5M dynamic emotion labels. Finally, we present MVEmo-Bench, a multi-task benchmark with evaluation metrics specifically designed for MRER tasks and the natural language outputs generated by large language models (LLMs). Our work makes a significant contribution to MRER by addressing key challenges and providing robust foundations for future research.</p>
          </div>

          <h2 id="annotation">MVAnno: Unified Emotion Annotation Framework</h2>
          <div style="text-align:center">
            <img src="../image/mvemo/annotation.png" style="width: 80%;">
          </div>

          <h2 id="dataset">MVEmo Dataset</h2>
          <div style="text-align:left">
            <p>The MVEmo dataset comprises 11,764 music video samples (7,923 with lyrics and 3,841 without lyrics), establishing it as a large-scale resource for multimodal emotion research. We conduct comprehensive analysis, including basic, modality-specific, and emotion-related analysis, based on the statistics of the MVEmo dataset.</p>
          </div>

          <script>
            (function (i, s, o, g, r, a, m) {
              i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
              }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
            ga('create', 'UA-139981676-1', 'auto');
            ga('send', 'pageview');
          </script>

          <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
          <script>hljs.initHighlightingOnLoad();</script>



          <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
         HTML: ["input/TeX","output/HTML-CSS"],
         TeX: {
                Macros: {
                         bm: ["\\boldsymbol{#1}", 1],
                         argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
                         argmin: ["\\mathop{\\rm arg\\,min}\\limits"]},
                extensions: ["AMSmath.js","AMSsymbols.js"],
                equationNumbers: { autoNumber: "AMS" } },
         extensions: ["tex2jax.js"],
         jax: ["input/TeX","output/HTML-CSS"],
         tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true },
         "HTML-CSS": { availableFonts: ["TeX"],
                       linebreaks: { automatic: true } }
     });
 </script>

          <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
       }
     });
 </script>

          <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
            </script>




</body>

</html>